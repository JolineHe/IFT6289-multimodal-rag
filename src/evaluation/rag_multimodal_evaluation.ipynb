{
 "cells": [
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "# 获取当前notebook的绝对路径\n",
    "current_dir = os.path.abspath('')\n",
    "# 获取项目根目录的路径（src的父目录）\n",
    "project_root = os.path.dirname(os.path.dirname(current_dir))\n",
    "sys.path.append(project_root)\n",
    "\n",
    "N_samp = 20\n",
    "alpha = 0.7\n",
    "\n",
    "from src.evaluation.evaluation_multimodal import *"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get random properties as test set"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "random_properties, random_property_ids = get_random_img_properties(N_samp)\n",
    "# random_properties[0], random_property_ids"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate queries for these properties"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "random_properties_and_queries = generate_queries(random_properties)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "# random_properties_and_queries[0]",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "sample_queries = [\n",
    "    {'text': property['generated_query'], 'files': [property['image_path']]}\n",
    "    for property in random_properties_and_queries\n",
    "]\n",
    "# print(sample_queries[0])\n",
    "# print(len(sample_queries))"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "from typing import Optional\n",
    "from pydantic import BaseModel, Field\n",
    "from typing import List, Dict, Any\n",
    "from src.data_models import Address, ImageDescrib\n",
    "import pandas as pd \n",
    "from src.multimodal_search import *\n",
    "\n",
    "\n",
    "collection = get_collection()\n",
    "hybrid_searcher = MultiModalSearch(collection)\n",
    "\n",
    "\n",
    "class SearchResultItem(BaseModel):\n",
    "    id: int = Field(alias='_id')\n",
    "    name: str\n",
    "    accommodates: Optional[int] = None\n",
    "    address: Address\n",
    "    summary: Optional[str] = None\n",
    "    description: Optional[str] = None\n",
    "    neighborhood_overview: Optional[str] = None\n",
    "    notes: Optional[str] = None\n",
    "    images: ImageDescrib\n",
    "    search_score: Optional[float] = None\n",
    "    reviews: Optional[List[Dict[str, Any]]] = None"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Retrive top 10 most similar listings from database for each property"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "retrieved_results = []\n",
    "for query in sample_queries:\n",
    "    results = hybrid_searcher.do_search(query,alpha_text=alpha)\n",
    "    search_results_models = [SearchResultItem(**result)  for result in results]\n",
    "    search_results_df = pd.DataFrame([item.model_dump() for item in search_results_models])\n",
    "    retrieved_results.append(search_results_df)\n",
    "\n",
    "    \n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "# (retrieved_results[0]).columns",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "retrieved_ids = []\n",
    "for result in retrieved_results:\n",
    "    retrieved_ids.append(result['id'].tolist())"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check if random property ids are in the top k retrieved ids"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "top_k_positions = []\n",
    "for i in range(len(retrieved_ids)):\n",
    "    top_k_positions.append(check_top_k_positions(retrieved_ids[i], random_property_ids[i]))\n",
    "    \n",
    "# top_k_positions"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Convert top_k_positions list to DataFrame\n",
    "top_k_df = pd.DataFrame(top_k_positions, columns=['top_1', 'top_3', 'top_5', 'top_10'])\n",
    "top_1 = sum(top_k_df['top_1'].tolist())/len(top_k_df)\n",
    "top_3 = sum(top_k_df['top_3'].tolist())/len(top_k_df)\n",
    "top_5 = sum(top_k_df['top_5'].tolist())/len(top_k_df)\n",
    "top_10 = sum(top_k_df['top_10'].tolist())/len(top_k_df)\n",
    "\n",
    "# top_1, top_3, top_5, top_10"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using Ragas to evaluate the response by LLM\n",
    "- Context Recall: \n",
    "- Faithfulness\n",
    "- Factual correctness"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Context Recall measures how many of the relevant documents (or pieces of information) were successfully retrieved. It focuses on not missing important results. Higher recall means fewer relevant documents were left out. In short, recall is about not missing anything important. Since it is about not missing anything, calculating context recall always requires a reference to compare against."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Faithfulness metric measures how factually consistent a response is with the retrieved context. It ranges from 0 to 1, with higher scores indicating better consistency."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "FactualCorrectness is a metric class that evaluates the factual correctness of responses generated by a language model. It uses claim decomposition and natural language inference (NLI) to verify the claims made in the responses against reference texts."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "expected_responses = []\n",
    "for i in range(len(sample_queries)):\n",
    "    expected_responses.append(random_properties[i]['description'])\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# generate ground truth image caption\n",
    "# random_properties[0]['image_url']"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "top_1_retrieved_contexts = []\n",
    "top_1_retrieved_contexts = [retrieved_results[i]['description'][0] for i in range(len(retrieved_results))]\n",
    "# top_1_retrieved_contexts\n",
    "# retrieved_results[0]['images'][0]['picture_url']"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "def generate_response(query, context):      \n",
    "    prompt_template = ChatPromptTemplate.from_messages([\n",
    "                (\"system\", \"\"\"You are an Airbnb listing recommendation system. Please:\n",
    "                1. Respond in the same language as the user\n",
    "                2. If the user is asking for property recommendations:\n",
    "                   - Prioritize results with higher search scores\n",
    "                   - Include the Airbnb listing URL and image URL\n",
    "                   - Explain why you chose these properties\n",
    "                   - Highlight features that match the user's criteria\n",
    "                3. If the user has provided an image, consider visual similarity in your recommendations\n",
    "                4. Be friendly and helpful in your responses\n",
    "                5, answer the question in the same language as the query\"\"\"),\n",
    "                (\"human\", \"Answer this user query: {query} with the following context:\\n{context}\")\n",
    "            ])\n",
    "\n",
    "    formatted_messages = prompt_template.format_messages(query=query, context=context)\n",
    "    return rag_llm(formatted_messages)  "
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "dataset = []\n",
    "\n",
    "for query, reference, context in zip(sample_queries, expected_responses, top_1_retrieved_contexts):\n",
    "    response = generate_response(query, context)\n",
    "    dataset.append(\n",
    "        {\n",
    "            \"user_input\":query['text'],\n",
    "            \"retrieved_contexts\": [context],\n",
    "            \"response\":response.content,\n",
    "            \"reference\":reference\n",
    "        }\n",
    "    )"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# print('--response: ',response.content)\n",
    "# print('--ground_truth: ', reference)\n",
    "# print('--input: ', query)\n",
    "# print('--top1_retrived_contexts: ', context)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "# dataset",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "from ragas import EvaluationDataset\n",
    "from ragas import evaluate\n",
    "from ragas.llms import LangchainLLMWrapper\n",
    "from ragas.metrics import LLMContextRecall, Faithfulness, FactualCorrectness\n",
    "\n",
    "\n",
    "evaluation_dataset = EvaluationDataset.from_list(dataset)\n",
    "evaluator_llm = LangchainLLMWrapper(ChatOpenAI(model=\"gpt-4.1-nano\"))"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "result = evaluate(dataset=evaluation_dataset,metrics=[LLMContextRecall(), Faithfulness(), FactualCorrectness()],llm=evaluator_llm)\n",
    "result\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "top_1, top_3, top_5, top_10",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "result.upload()",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp_course_prj",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
